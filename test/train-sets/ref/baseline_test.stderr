Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
Enabled reductions: gd, baseline, scorer
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       51
0.648807 0.297614            2            2.0   0.0000   0.5455      104
0.397826 0.146845            4            4.0   0.0000   0.3158      135
0.295379 0.192932            8            8.0   0.0000   0.2760      146
0.253704 0.212029           16           16.0   1.0000   0.4438       24
0.237305 0.220907           32           32.0   0.0000   0.3310       32
0.231645 0.225984           64           64.0   0.0000   0.1647       61
0.221327 0.211009          128          128.0   1.0000   0.8441      106

finished run
number of examples = 200
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.190245
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
